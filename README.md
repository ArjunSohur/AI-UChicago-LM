### AI@UChicago - Languge Models

The goal of this mini-course is to teach people the absolute basics of language models, or at least enough so that they can start their own projects and research. \

We will be looking at both theory and code in order to better understand both how LMs operate and how to operate them.  That being said, there is still a LOT more to learn than we will be teaching.  What we will be covering is:

1. Attention
2. The Transformer
3. Transformer Implementation
4. Fine tuning an LLM
5. LLM landscape

Some of these topics will require a grasp of certain linear algebra concepts, like matrix multiplication and dot product.  We're not really going to teach you these concepts, but the operations themselves aren't too heard to learn.
Here are some resources that explain the basics:
 - https://www.youtube.com/watch?v=2spTnAiQg4M
 - https://en.wikipedia.org/wiki/Matrix_multiplication
 - https://en.wikipedia.org/wiki/Dot_product

We're also going to assume some familiarity with machine learning, back propagation, and loss.

Now, do you *need* these things to get something out of the course.  No, not really.  But do you *need* these things to deeply understand what we'll be teaching.  Probably.

I'll leave it at this: we don't have any actual prerequisites and we try to make the class accessible to all.  Your final grasp on the subject, however, will depend on your knowledge of
matricies, neural networks, and the concept of machine learning.

If there are any questions, let us know.
