### Words as numbers

How language is represented

Tokens
* Naive way of assigning words to text
* Breaking down words into further words
* Breaking them down even further into tokens

Why tokens aren't the best for us
* That's just a random number - we can assign it any way we want
* Even if we get a good assignment - it's still just a number; it's hard to do math with

Vectors
*  We like vectors - a lot more information
*  We want our vectors to represent the semantic relations of words - adding, subtracting, cosine similarity
*  It's not perfect perfect semantic to math, but it's pretty good
*  Cosine similarity formula
*  Man/king woman/queen example

LLM inputs
* word vectors are important becasue they consititue the inputs of language models
* n token sentence
