## Words as numbers

### Slide 1
### Slide 2
How language is represented

### Slide 3
#### **Tokens**
* Naive way of assigning words to text
* Breaking down words into further words
* Breaking them down even further into tokens

#### **Why tokens aren't the best for us**
* That's just a random number - we can assign it any way we want
* Even if we get a good assignment - it's still just a number; it's hard to do math with


### Slide 4
#### **Vectors**
*  We like vectors - a lot more information
*  We want our vectors to represent the semantic relations of words - adding, subtracting, cosine similarity
*  It's not perfect perfect semantic to math, but it's pretty good
*  Cosine similarity formula
*  Man/king woman/queen example

##### **LLM inputs**
* word vectors are important becasue they consititue the inputs of language models
* n token sentence

### Slide 5
### Intro to attention - intuition

### Slide 6
#### **Little game** 
* Play it with audience
  
### Slide 7
#### **Machines can play too** 
* Discuss probabilities
* Humans play by looking at the words surrounding the mask - machines do the same thing
* Machines pay ATTENTION to the different words

### Slide 8
#### **Attention matricies**
* Talk about how it's not perfect
* Query and Key language

### Slide 9
### Intro to attention - math

### Slide 10
#### **QKV intuition**
* Dicitonaries - explain each component

### Slide 11
#### **QKV math**

1)
*   $n$ query vectors $q_i$, with dimension $d_q$
*   $n$ key vectors, $k_i$, with dimension $d_q$
*   $n$ corresponsing value vectors, $v_i$, with dimension $d_v$

Note that we don't necessarily need $d_q = d_v$, but we abolutely need the key and query vectors to be the same dimension.

2)
In order to understand the compatibility between the queries and keys, we need to multiply the matricies, which we do by $QK^T$.

Normalization

Thus we have $\frac{QK^T}{\sqrt{d_q}} \in \mathbb{R}^{n \ \times \ n }$.

**"softmax"**, denoted $\sigma:\mathbb{R}^n \rightarrow (0, 1]^n$

<img width="225" alt="Screenshot 2024-07-09 at 6 42 50â€¯PM" src="https://github.com/ArjunSohur/AI-UChicago-LM/assets/105809809/f9af999f-3f7f-4532-9e28-73fd0aa23fc6">

We apply softmax row-wise to obtain: $$\sigma(\frac{QK^T}{\sqrt{d_q}})\in \mathbb{R}^{n \ \times \ n }$$

Explain why softmax and regular normalization is needed

3)
perform matrix multiplication with $V$

So, finally, we get: $$Attention(Q, K, V) = \sigma(\frac{QK^T}{\sqrt{d_q}})V \in \mathbb{R}^{n \ \times \ d_v }$$

**This is our formula for scaled dot product attention.**


  
